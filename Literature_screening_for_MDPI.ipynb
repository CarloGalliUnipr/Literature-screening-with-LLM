{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQjFvCYm_zJv"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PACKAGE INSTALLATION\n",
        "# =============================================================================\n",
        "# Install necessary packages. (Run these in your environment if not already installed.)\n",
        "# You should choose GPU runtime if possible\n",
        "!pip install biopython sentence-transformers pandas numpy requests openai\n",
        "\n",
        "# =============================================================================\n",
        "# IMPORTS AND INITIAL SETUP\n",
        "# =============================================================================\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from Bio import Entrez, Medline\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import openai\n",
        "\n",
        "# Set your email (required by NCBI) and your OpenAI API key.\n",
        "Entrez.email = \"your.email@example.com\"  # Replace with your email address.\n",
        "openai.api_key = \"YOUR_OPENAI_API_KEY\"     # Replace with your OpenAI API key.\n",
        "\n",
        "# =============================================================================\n",
        "# PUBMED DATA FETCHING AND PARSING\n",
        "# =============================================================================\n",
        "# Define a list of target PMIDs (these are the 9 papers we used in our publications, update with your own PMIDs as needed)\n",
        "target_pmids = [\n",
        "    \"11990444\", \"12186348\", \"11990441\", \"19053917\",\n",
        "    \"20054593\", \"23484181\", \"23379539\", \"26556577\", \"31811645\"\n",
        "]\n",
        "\n",
        "def fetch_pubmed_records(pmids):\n",
        "    \"\"\"\n",
        "    Given a list of PMIDs, fetch the corresponding PubMed records and return them as a list of Medline records.\n",
        "    \"\"\"\n",
        "    handle = Entrez.efetch(db=\"pubmed\", id=pmids, rettype=\"medline\", retmode=\"text\")\n",
        "    records = list(Medline.parse(handle))\n",
        "    handle.close()\n",
        "    return records\n",
        "\n",
        "def parse_pubmed_records(records):\n",
        "    \"\"\"\n",
        "    Parse Medline records to extract Title, Abstract, Year, and Journal.\n",
        "    Returns a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for record in records:\n",
        "        title = record.get(\"TI\", \"\")\n",
        "        abstract = record.get(\"AB\", \"\")\n",
        "        journal = record.get(\"JT\", \"\")\n",
        "        # Extract publication year from the date (if available)\n",
        "        pub_date = record.get(\"DP\", \"\")\n",
        "        year_match = re.search(r\"\\b(19|20)\\d{2}\\b\", pub_date)\n",
        "        year = year_match.group(0) if year_match else \"\"\n",
        "        data.append({\n",
        "            \"Title\": title,\n",
        "            \"Abstract\": abstract,\n",
        "            \"Year\": year,\n",
        "            \"Journal\": journal\n",
        "        })\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Fetch target records and parse them.\n",
        "target_records = fetch_pubmed_records(target_pmids)\n",
        "data_target = parse_pubmed_records(target_records)\n",
        "data_target[\"Target\"] = 1\n",
        "data_target[\"PMID\"] = target_pmids\n",
        "\n",
        "# Save target data to a generic location.\n",
        "data_target.to_csv(\"data/target.csv\", index=False)\n",
        "\n",
        "# =============================================================================\n",
        "# RETRIEVE ADDITIONAL PMIDs FROM PUBMED BY YEAR (CHUNKING)\n",
        "# =============================================================================\n",
        "def esearch_query(query, mindate, maxdate, retstart=0, retmax=10000):\n",
        "    \"\"\"\n",
        "    Perform an ESearch call on PubMed for a given date range.\n",
        "    \"\"\"\n",
        "    handle = Entrez.esearch(\n",
        "        db=\"pubmed\",\n",
        "        term=query,\n",
        "        mindate=str(mindate),\n",
        "        maxdate=str(maxdate),\n",
        "        retstart=retstart,\n",
        "        retmax=retmax\n",
        "    )\n",
        "    results = Entrez.read(handle)\n",
        "    handle.close()\n",
        "    return results\n",
        "\n",
        "def fetch_pmids_by_year_chunks(query, start_year, end_year, step=5):\n",
        "    \"\"\"\n",
        "    Partition the query by date ranges and return a set of unique PMIDs.\n",
        "    \"\"\"\n",
        "    all_pmids = set()\n",
        "    current_year = start_year\n",
        "    while current_year <= end_year:\n",
        "        chunk_start = current_year\n",
        "        chunk_end = min(current_year + step - 1, end_year)\n",
        "        print(f\"Searching {chunk_start} to {chunk_end}\")\n",
        "        chunk_search = esearch_query(query, chunk_start, chunk_end, retmax=0)\n",
        "        chunk_count = int(chunk_search[\"Count\"])\n",
        "        if chunk_count == 0:\n",
        "            current_year += step\n",
        "            continue\n",
        "        print(f\"  Found {chunk_count} articles in {chunk_start}-{chunk_end}\")\n",
        "        batch_size = 10000\n",
        "        for start in range(0, chunk_count, batch_size):\n",
        "            sub_results = esearch_query(query, chunk_start, chunk_end, retstart=start, retmax=batch_size)\n",
        "            pmids_batch = sub_results[\"IdList\"]\n",
        "            all_pmids.update(pmids_batch)\n",
        "            print(f\"    Retrieved {len(pmids_batch)} PMIDs; total so far {len(all_pmids)}\")\n",
        "        current_year += step\n",
        "    return all_pmids\n",
        "\n",
        "# Example usage: searching for articles on \"periodontal regeneration\"\n",
        "search_query = \"periodontal regeneration\"\n",
        "start_year = 1970\n",
        "end_year = 2025\n",
        "pmid_set = fetch_pmids_by_year_chunks(query=search_query, start_year=start_year, end_year=end_year, step=5)\n",
        "print(\"Total unique PMIDs retrieved:\", len(pmid_set))\n",
        "print(\"Sample PMIDs:\", list(pmid_set)[:10])\n",
        "\n",
        "def fetch_pmids_in_batches(pmids, batch_size=200):\n",
        "    \"\"\"\n",
        "    Batch-fetch PubMed records for a list of PMIDs.\n",
        "    \"\"\"\n",
        "    all_records = []\n",
        "    pmids_list = list(pmids)\n",
        "    for start_idx in range(0, len(pmids_list), batch_size):\n",
        "        batch_pmids = pmids_list[start_idx:start_idx+batch_size]\n",
        "        records = fetch_pubmed_records(batch_pmids)\n",
        "        all_records.extend(records)\n",
        "    return all_records\n",
        "\n",
        "# Fetch records from the additional PMIDs.\n",
        "pmid_list = list(pmid_set)\n",
        "pool_records = fetch_pmids_in_batches(pmid_list, batch_size=200)\n",
        "data_pool = parse_pubmed_records(pool_records)\n",
        "\n",
        "# Remove any duplicates already in the target set.\n",
        "data_pool = data_pool[~data_pool[\"PMID\"].isin(set(data_target[\"PMID\"]))]\n",
        "data_pool[\"Target\"] = 0\n",
        "\n",
        "# =============================================================================\n",
        "# EMBEDDINGS AND SIMILARITY CALCULATION\n",
        "# =============================================================================\n",
        "# Load the sentence transformer model for encoding article texts.\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "embedding_model = SentenceTransformer(model_name)\n",
        "\n",
        "def encode_text(df):\n",
        "    \"\"\"\n",
        "    Concatenate the Title and Abstract columns and encode them into embeddings.\n",
        "    \"\"\"\n",
        "    texts = (df[\"Title\"] + \" \" + df[\"Abstract\"]).tolist()\n",
        "    embeddings = embedding_model.encode(texts, convert_to_tensor=True)\n",
        "    return embeddings\n",
        "\n",
        "target_embeddings = encode_text(data_target)\n",
        "pool_embeddings = encode_text(data_pool)\n",
        "print(\"target_embeddings shape:\", target_embeddings.shape)\n",
        "print(\"pool_embeddings shape:\", pool_embeddings.shape)\n",
        "\n",
        "# Compute the mean embedding of target articles.\n",
        "mean_target_embedding = target_embeddings.mean(dim=0, keepdim=True)\n",
        "similarities = util.cos_sim(pool_embeddings, mean_target_embedding).squeeze()\n",
        "data_pool[\"similarity\"] = similarities.cpu().numpy()\n",
        "\n",
        "# =============================================================================\n",
        "# DIVIDE THE POOL INTO QUARTILES BASED ON SIMILARITY\n",
        "# =============================================================================\n",
        "q1 = data_pool[\"similarity\"].quantile(0.25)\n",
        "q2 = data_pool[\"similarity\"].quantile(0.50)\n",
        "q3 = data_pool[\"similarity\"].quantile(0.75)\n",
        "\n",
        "df_q1 = data_pool[data_pool[\"similarity\"] <= q1].copy()\n",
        "df_q2 = data_pool[(data_pool[\"similarity\"] > q1) & (data_pool[\"similarity\"] <= q2)].copy()\n",
        "df_q3 = data_pool[(data_pool[\"similarity\"] > q2) & (data_pool[\"similarity\"] <= q3)].copy()\n",
        "df_q4 = data_pool[data_pool[\"similarity\"] > q3].copy()\n",
        "\n",
        "print(\"Quartile shapes:\", df_q1.shape, df_q2.shape, df_q3.shape, df_q4.shape)\n",
        "print(\"Mean similarities:\", df_q1[\"similarity\"].mean(), df_q2[\"similarity\"].mean(),\n",
        "      df_q3[\"similarity\"].mean(), df_q4[\"similarity\"].mean())\n",
        "\n",
        "def merge_and_save(data_target, df_quartile, quartile_label, out_dir=\"data/\"):\n",
        "    \"\"\"\n",
        "    Merge target records with a given quartile subset and save to CSV.\n",
        "    \"\"\"\n",
        "    merged_df = pd.concat([data_target, df_quartile], ignore_index=True)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    filename = os.path.join(out_dir, f\"merged_quartile_{quartile_label}.csv\")\n",
        "    merged_df.to_csv(filename, index=False)\n",
        "    print(f\"Saved {filename} with shape {merged_df.shape}.\")\n",
        "\n",
        "# Save merged quartile files.\n",
        "merge_and_save(data_target, df_q1, \"1\")\n",
        "merge_and_save(data_target, df_q2, \"2\")\n",
        "merge_and_save(data_target, df_q3, \"3\")\n",
        "merge_and_save(data_target, df_q4, \"4\")\n",
        "\n",
        "# =============================================================================\n",
        "# CREATE SUB-SAMPLES FROM MERGED DATA\n",
        "# =============================================================================\n",
        "input_files = [\n",
        "    \"data/merged_quartile_1.csv\",\n",
        "    \"data/merged_quartile_2.csv\",\n",
        "    \"data/merged_quartile_3.csv\",\n",
        "    \"data/merged_quartile_4.csv\",\n",
        "]\n",
        "output_files = [\n",
        "    \"data/red_merged_quartile_1.csv\",\n",
        "    \"data/red_merged_quartile_2.csv\",\n",
        "    \"data/red_merged_quartile_3.csv\",\n",
        "    \"data/red_merged_quartile_4.csv\",\n",
        "]\n",
        "\n",
        "total_sample_size = 200\n",
        "\n",
        "for input_file, output_file in zip(input_files, output_files):\n",
        "    df = pd.read_csv(input_file)\n",
        "    # Separate target (label 1) and non-target (label 0) articles.\n",
        "    target_1_df = df[df[\"Target\"] == 1]\n",
        "    target_0_df = df[df[\"Target\"] == 0]\n",
        "    if len(target_1_df) > total_sample_size:\n",
        "        raise ValueError(\"There are more than 200 target articles. Adjust the sample size.\")\n",
        "    remaining_sample_size = total_sample_size - len(target_1_df)\n",
        "    target_0_sample = target_0_df.sample(n=remaining_sample_size, random_state=42)\n",
        "    reduced_df = pd.concat([target_1_df, target_0_sample], ignore_index=True)\n",
        "    reduced_df = reduced_df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "    reduced_df.to_csv(output_file, index=False)\n",
        "    print(f\"Saved {output_file} with shape {reduced_df.shape}\")\n",
        "\n",
        "# =============================================================================\n",
        "# GPT-3.5 TURBO CLASSIFICATION\n",
        "# =============================================================================\n",
        "# Below are two functions that use GPT-3.5 Turbo to classify articles.\n",
        "# We used a \"soft approach\" prompt.\n",
        "\n",
        "def classify_article_with_gpt3_soft(title, abstract):\n",
        "    \"\"\"\n",
        "    Zero-shot classification using GPT-3.5 Turbo with a soft-approach prompt.\n",
        "    Returns \"ACCEPT\" if the article meets the criteria, otherwise \"REJECT\".\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "You are assisting in a systematic review on periodontal regeneration comparing\n",
        "Emdogain (EMD) + bone graft (BG) versus BG alone.\n",
        "\n",
        "Your task is to decide whether the following article should be **ACCEPTED**\n",
        "or **REJECTED** based on the following “soft approach” criteria:\n",
        "\n",
        "**Inclusion Criteria**:\n",
        "1. Population: Adult periodontitis patients (≥18 years) with at least one intrabony or furcation defect.\n",
        "2. Intervention: Regenerative surgical procedures involving EMD combined with any bone graft material (EMD+BG).\n",
        "3. Comparison: Surgical procedures using bone graft alone.\n",
        "4. Outcomes: Primary outcomes like CAL gain and PD reduction; secondary outcomes may include pocket closure, wound healing, gingival recession, tooth loss, PROMs, or adverse events.\n",
        "5. Study Design: RCT (parallel or split-mouth), with ≥10 patients per arm and ≥6 months follow-up.\n",
        "\n",
        "**Soft Approach**:\n",
        "- If at least one of these criteria is explicitly met or strongly implied, and none are contradicted, then respond with ACCEPT.\n",
        "- Otherwise, respond with REJECT.\n",
        "\n",
        "Title: {title}\n",
        "Abstract: {abstract}\n",
        "\n",
        "Respond with exactly:\n",
        "ACCEPT\n",
        "or\n",
        "REJECT\n",
        "\"\"\"\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.0,\n",
        "            max_tokens=64\n",
        "        )\n",
        "        answer = response[\"choices\"][0][\"message\"][\"content\"].strip().upper()\n",
        "        return \"ACCEPT\" if \"ACCEPT\" in answer else \"REJECT\"\n",
        "    except Exception as e:\n",
        "        print(f\"OpenAI API error: {e}\")\n",
        "        return \"REJECT\"\n",
        "\n",
        "\n",
        "# Process reduced CSV files using both GPT-3.5 Turbo classification functions.\n",
        "reduced_files = [\n",
        "    \"data/red_merged_quartile_1.csv\",\n",
        "    \"data/red_merged_quartile_2.csv\",\n",
        "    \"data/red_merged_quartile_3.csv\",\n",
        "    \"data/red_merged_quartile_4.csv\",\n",
        "]\n",
        "\n",
        "# Directory to store GPT-3.5 Turbo scored results.\n",
        "gpt3_output_dir = \"data/GPT3/\"\n",
        "os.makedirs(gpt3_output_dir, exist_ok=True)\n",
        "\n",
        "# --- Soft Approach ---\n",
        "for csv_file in reduced_files:\n",
        "    print(f\"\\nProcessing file (soft approach): {csv_file}\")\n",
        "    df = pd.read_csv(csv_file)\n",
        "    df[\"Title\"] = df[\"Title\"].fillna(\"\")\n",
        "    df[\"Abstract\"] = df[\"Abstract\"].fillna(\"\")\n",
        "    decisions = []\n",
        "    for _, row in df.iterrows():\n",
        "        decision = classify_article_with_gpt3_soft(row[\"Title\"], row[\"Abstract\"])\n",
        "        decisions.append(1 if decision == \"ACCEPT\" else 0)\n",
        "    df[\"Accepted_GPT3\"] = decisions\n",
        "    output_filename = os.path.join(gpt3_output_dir, os.path.basename(csv_file).replace(\".csv\", \"_gpt3_scored.csv\"))\n",
        "    df.to_csv(output_filename, index=False)\n",
        "    print(f\"Saved GPT-3.5 Turbo (soft) scored file to: {output_filename}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# END OF SCRIPT\n",
        "# =============================================================================\n"
      ]
    }
  ]
}